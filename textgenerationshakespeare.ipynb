{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "textgenerationshakespeare.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOV93I0Ov6PX81qnwZkYgT4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DhayalanCS/TensorFlow/blob/main/textgenerationshakespeare.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iBIscMS92uQ4"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers.experimental import preprocessing\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "import time"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VN93OoRO7ill",
        "outputId": "61ac583b-2784-4d38-c71b-c7846a84b7b8"
      },
      "source": [
        "path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\n",
            "1122304/1115394 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "80TqKKa68gwo",
        "outputId": "db4cdce7-898e-41b8-d1fa-770f8179c6ec"
      },
      "source": [
        "#decoding the text from the file\n",
        "text = open(path_to_file, 'rb').read().decode(encoding ='utf-8')\n",
        "print(f'Length of text: {len(text)} characters')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Length of text: 1115394 characters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "adxlE1qA9l7U",
        "outputId": "3934aa95-f928-49b5-9aab-0f7a2736a61b"
      },
      "source": [
        "print(text[:300]) #first300characters"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n",
            "All:\n",
            "We know't, we know't.\n",
            "\n",
            "First Citizen:\n",
            "Let us\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RSMTKVTU97sj",
        "outputId": "3a162b16-c2f1-42fb-da53-40241676f9e1"
      },
      "source": [
        "#unique characters in the file\n",
        "vocab = sorted(set(text))\n",
        "print(f'{len(vocab)} unique characters')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "65 unique characters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sSlbCGdc-Ywf",
        "outputId": "5368606f-42be-4807-b6bf-ea6592f42237"
      },
      "source": [
        "# here we are processing the text by vectorizing it. The words are given a numerical id.\n",
        "example_text = ['abcdefg','xyz']\n",
        "chars = tf.strings.unicode_split(example_text, input_encoding='UTF-8')\n",
        "chars"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[b'a', b'b', b'c', b'd', b'e', b'f', b'g'], [b'x', b'y', b'z']]>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fSpL81XY_Rrj",
        "outputId": "104e5e86-c0aa-447b-9c91-824dc75fc3b4"
      },
      "source": [
        "#creating the preprocessing layer\n",
        "#tokens to character ids\n",
        "ids_from_chars = preprocessing.StringLookup(\n",
        "    vocabulary = list(vocab)\n",
        ")\n",
        "\n",
        "ids = ids_from_chars(chars)\n",
        "ids"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[41, 42, 43, 44, 45, 46, 47], [64, 65, 66]]>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tlCBtDKx__rr"
      },
      "source": [
        "chars_from_ids = tf.keras.layers.experimental.preprocessing.StringLookup(\n",
        "    vocabulary = ids_from_chars.get_vocabulary(), invert=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kjNxU6YpnDT2",
        "outputId": "60f72b27-cbc5-4413-db53-979e9d40da3b"
      },
      "source": [
        "chars = chars_from_ids(ids)\n",
        "chars"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[b'a', b'b', b'c', b'd', b'e', b'f', b'g'], [b'x', b'y', b'z']]>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J8pFiq1dnTA4",
        "outputId": "c8c14eb5-33f8-4060-845a-db1db954311f"
      },
      "source": [
        "tf.strings.reduce_join(chars, axis= -1).numpy()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([b'abcdefg', b'xyz'], dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JeXDAJtTnhMX"
      },
      "source": [
        "def text_from_ids(ids):\n",
        "  return tf.strings.reduce_join(chars_from_ids(ids), axis = -1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eaVj_Bm5n4Kh",
        "outputId": "2ccd68fd-d23e-47f2-aa33-9923c33b4d19"
      },
      "source": [
        "#the prediction task\n",
        "all_ids = ids_from_chars(tf.strings.unicode_split(text, 'UTF-8'))\n",
        "all_ids"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(1115394,), dtype=int64, numpy=array([20, 49, 58, ..., 47, 10,  2])>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L8G5zQKErPRm",
        "outputId": "66c75df5-39ce-43d0-d6c8-2595e214c73f"
      },
      "source": [
        "ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids)\n",
        "#tensor slices function to convert the text vector into a stream of character indices.\n",
        "\n",
        "for ids in ids_dataset.take(15):\n",
        "  print(chars_from_ids(ids).numpy().decode('utf-8'))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "F\n",
            "i\n",
            "r\n",
            "s\n",
            "t\n",
            " \n",
            "C\n",
            "i\n",
            "t\n",
            "i\n",
            "z\n",
            "e\n",
            "n\n",
            ":\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K2tV0SVNr9-V",
        "outputId": "bf490bdc-f737-4488-fbe9-5fb5dba902d1"
      },
      "source": [
        "#batch method lets you easily convert these individual characters to sequences of the desired size.\n",
        "\n",
        "seq_length = 100\n",
        "example_per_epoch = len(text)//(seq_length+1)\n",
        "\n",
        "sequences = ids_dataset.batch(seq_length+1, drop_remainder=True)\n",
        "\n",
        "for seq in sequences.take(1):\n",
        "  print(chars_from_ids(seq))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(\n",
            "[b'F' b'i' b'r' b's' b't' b' ' b'C' b'i' b't' b'i' b'z' b'e' b'n' b':'\n",
            " b'\\n' b'B' b'e' b'f' b'o' b'r' b'e' b' ' b'w' b'e' b' ' b'p' b'r' b'o'\n",
            " b'c' b'e' b'e' b'd' b' ' b'a' b'n' b'y' b' ' b'f' b'u' b'r' b't' b'h'\n",
            " b'e' b'r' b',' b' ' b'h' b'e' b'a' b'r' b' ' b'm' b'e' b' ' b's' b'p'\n",
            " b'e' b'a' b'k' b'.' b'\\n' b'\\n' b'A' b'l' b'l' b':' b'\\n' b'S' b'p' b'e'\n",
            " b'a' b'k' b',' b' ' b's' b'p' b'e' b'a' b'k' b'.' b'\\n' b'\\n' b'F' b'i'\n",
            " b'r' b's' b't' b' ' b'C' b'i' b't' b'i' b'z' b'e' b'n' b':' b'\\n' b'Y'\n",
            " b'o' b'u' b' '], shape=(101,), dtype=string)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zEhfJBYUsXT5",
        "outputId": "ba7f0793-9445-47c6-c8a5-566afb1a431d"
      },
      "source": [
        "for seq in sequences.take(5):\n",
        "  print(text_from_ids(seq).numpy())\n",
        "  "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "b'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n",
            "b'are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you k'\n",
            "b\"now Caius Marcius is chief enemy to the people.\\n\\nAll:\\nWe know't, we know't.\\n\\nFirst Citizen:\\nLet us ki\"\n",
            "b\"ll him, and we'll have corn at our own price.\\nIs't a verdict?\\n\\nAll:\\nNo more talking on't; let it be d\"\n",
            "b'one: away, away!\\n\\nSecond Citizen:\\nOne word, good citizens.\\n\\nFirst Citizen:\\nWe are accounted poor citi'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t2OXk_qCtzQX"
      },
      "source": [
        "#creating training examples and targets\n",
        "def split_input_target(sequence):\n",
        "    input_text = sequence[:-1]\n",
        "    target_text = sequence[1:]\n",
        "    return input_text, target_text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qTyFH7zJuAdG",
        "outputId": "27d3fd79-0eab-4373-bfe8-aeffacd59b20"
      },
      "source": [
        "split_input_target(list(\"Tensorflow\"))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(['T', 'e', 'n', 's', 'o', 'r', 'f', 'l', 'o'],\n",
              " ['e', 'n', 's', 'o', 'r', 'f', 'l', 'o', 'w'])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-EKSqA0muMOJ",
        "outputId": "4b33e121-1edc-45c8-ad85-7239e6cd36d8"
      },
      "source": [
        "dataset = sequences.map(split_input_target)\n",
        "for input_example, target_example in dataset.take(1):\n",
        "    print(\"Input :\", text_from_ids(input_example).numpy())\n",
        "    print(\"Target:\", text_from_ids(target_example).numpy())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input : b'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou'\n",
            "Target: b'irst Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qBrhfcWZuYEe",
        "outputId": "b29e33b8-1ab6-43ed-a3dc-9c998ceb9a59"
      },
      "source": [
        "# Batch size\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "# Buffer size to shuffle the dataset\n",
        "# (TF data is designed to work with possibly infinite sequences,\n",
        "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
        "# it maintains a buffer in which it shuffles elements).\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "dataset = (\n",
        "    dataset\n",
        "    .shuffle(BUFFER_SIZE)\n",
        "    .batch(BATCH_SIZE, drop_remainder=True)\n",
        "    .prefetch(tf.data.experimental.AUTOTUNE))\n",
        "\n",
        "dataset"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<PrefetchDataset shapes: ((64, 100), (64, 100)), types: (tf.int64, tf.int64)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VzUD5cRiuZAm"
      },
      "source": [
        "# Length of the vocabulary in chars\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "# The embedding dimension\n",
        "embedding_dim = 256\n",
        "\n",
        "# Number of RNN units\n",
        "rnn_units = 5024\n",
        "\n",
        "#This model has three layers:\n",
        "\n",
        "#tf.keras.layers.Embedding: The input layer. A trainable lookup table that will map each character-ID to a vector with embedding_dim dimensions;\n",
        "#tf.keras.layers.GRU: A type of RNN with size units=rnn_units (You can also use an LSTM layer here.)\n",
        "#tf.keras.layers.Dense: The output layer, with vocab_size outputs. It outputs one logit for each character in the vocabulary. These are the log-likelihood of each character according to the model.\n",
        "\n",
        "class MyModel(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, rnn_units):\n",
        "    super().__init__(self)\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.gru = tf.keras.layers.GRU(rnn_units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True)\n",
        "    self.dense = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "  def call(self, inputs, states=None, return_state=False, training=False):\n",
        "    x = inputs\n",
        "    x = self.embedding(x, training=training)\n",
        "    if states is None:\n",
        "      states = self.gru.get_initial_state(x)\n",
        "    x, states = self.gru(x, initial_state=states, training=training)\n",
        "    x = self.dense(x, training=training)\n",
        "\n",
        "    if return_state:\n",
        "      return x, states\n",
        "    else:\n",
        "      return x\n",
        "\n",
        "\n",
        "model = MyModel(\n",
        "    # Be sure the vocabulary size matches the `StringLookup` layers.\n",
        "    vocab_size=len(ids_from_chars.get_vocabulary()),\n",
        "    embedding_dim=embedding_dim,\n",
        "    rnn_units=rnn_units)\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xYU_5eyhvAKb",
        "outputId": "bb9c2285-863f-4494-f778-080b95938b85"
      },
      "source": [
        "#checking the shape of the output\n",
        "for input_example_batch, target_example_batch in dataset.take(1):\n",
        "    example_batch_predictions = model(input_example_batch)\n",
        "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(64, 100, 67) # (batch_size, sequence_length, vocab_size)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xvoGfZt3vHMl",
        "outputId": "f218597d-9bc8-49cf-fb6c-06d8f615e116"
      },
      "source": [
        "model.summary()    \n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"my_model\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        multiple                  17152     \n",
            "_________________________________________________________________\n",
            "gru (GRU)                    multiple                  3938304   \n",
            "_________________________________________________________________\n",
            "dense (Dense)                multiple                  68675     \n",
            "=================================================================\n",
            "Total params: 4,024,131\n",
            "Trainable params: 4,024,131\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9JDSfeRqv9rr"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9f4rSG5gv-zz"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0S-reS6cvwkU",
        "outputId": "66ead290-2c8c-4719-8b3c-47336abcf919"
      },
      "source": [
        "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
        "sampled_indices = tf.squeeze(sampled_indices, axis=-1).numpy()\n",
        "\n",
        "sampled_indices"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([40, 44, 48, 39, 47,  2, 51, 22, 49, 46, 17, 28, 13, 53, 21, 57, 51,\n",
              "       31, 36,  2, 58, 53, 42, 37, 23, 25, 28, 14, 19, 10, 19, 53, 19, 22,\n",
              "       57, 31, 14, 16, 29, 63, 37, 27, 37, 35,  2, 22, 40, 53, 25,  2, 12,\n",
              "        8, 63,  7, 13, 25, 15, 33, 27, 34, 62, 55, 63, 20, 15,  9, 64, 19,\n",
              "       27, 61, 42,  1, 39,  0, 55, 33, 22, 27, 30, 38, 42, 61, 64, 39, 26,\n",
              "       18, 52,  8, 53, 27, 19, 35, 26, 14, 22,  9, 38, 29, 47,  6])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TZH6RoqJv4I2",
        "outputId": "622c956c-3a19-48ef-833f-beea36a2fb71"
      },
      "source": [
        "print(\"Input:\\n\", text_from_ids(input_example_batch[0]).numpy())\n",
        "print()\n",
        "print(\"Next Char Predictions:\\n\", text_from_ids(sampled_indices).numpy())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input:\n",
            " b\"our dies,\\nOr my shamed life in his dishonour lies:\\nThou kill'st me in his life; giving him breath,\\nT\"\n",
            "\n",
            "Next Char Predictions:\n",
            " b\"ZdhYg\\nkHifCN;mGqkQV\\nrmbWIKN?E.EmEHqQ?BOwWMWU\\nHZmK\\n:,w';KASMTvowFA-xEMub[UNK]YoSHMPXbuxYLDl,mMEUL?H-XOg&\"\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LQCYgILRwD2t"
      },
      "source": [
        "Train the model\n",
        "At this point the problem can be treated as a standard classification problem. Given the previous RNN state, and the input this time step, predict the class of the next character.\n",
        "\n",
        "Attach an optimizer, and a loss function\n",
        "The standard tf.keras.losses.sparse_categorical_crossentropy loss function works in this case because it is applied across the last dimension of the predictions.\n",
        "\n",
        "Because your model returns logits, you need to set the from_logits flag."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "atiCi7rNv_pe",
        "outputId": "76ee41d2-3916-4e70-b948-e0a57f0a6fc9"
      },
      "source": [
        "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "example_batch_loss = loss(target_example_batch, example_batch_predictions)\n",
        "mean_loss = example_batch_loss.numpy().mean()\n",
        "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
        "print(\"Mean loss:        \", mean_loss)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Prediction shape:  (64, 100, 67)  # (batch_size, sequence_length, vocab_size)\n",
            "Mean loss:         4.2048364\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TO0l0xsEwJt7",
        "outputId": "faef350e-2e76-406f-deed-8d8326865584"
      },
      "source": [
        "tf.exp(mean_loss).numpy()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "67.00963"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ubsvaJk_w9Ua"
      },
      "source": [
        "model.compile(optimizer='adam', loss=loss)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4XPwaeEoxBYl"
      },
      "source": [
        "# Directory where the checkpoints will be saved\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "# Name of the checkpoint files\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weights_only=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bCaLAlD9xF7l",
        "outputId": "757f0490-3ef1-43d0-88ae-a7a441119912"
      },
      "source": [
        "EPOCHS = 20\n",
        "\n",
        "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "172/172 [==============================] - 894s 5s/step - loss: 3.2955\n",
            "Epoch 2/20\n",
            "172/172 [==============================] - 895s 5s/step - loss: 2.0778\n",
            "Epoch 3/20\n",
            "172/172 [==============================] - 895s 5s/step - loss: 1.7565\n",
            "Epoch 4/20\n",
            "172/172 [==============================] - 888s 5s/step - loss: 1.5726\n",
            "Epoch 5/20\n",
            "172/172 [==============================] - 890s 5s/step - loss: 1.4573\n",
            "Epoch 6/20\n",
            "172/172 [==============================] - 883s 5s/step - loss: 1.3844\n",
            "Epoch 7/20\n",
            "172/172 [==============================] - 883s 5s/step - loss: 1.3287\n",
            "Epoch 8/20\n",
            "172/172 [==============================] - 883s 5s/step - loss: 1.2779\n",
            "Epoch 9/20\n",
            "172/172 [==============================] - 878s 5s/step - loss: 1.2361\n",
            "Epoch 10/20\n",
            "172/172 [==============================] - 886s 5s/step - loss: 1.1933\n",
            "Epoch 11/20\n",
            "172/172 [==============================] - 873s 5s/step - loss: 1.1481\n",
            "Epoch 12/20\n",
            "172/172 [==============================] - 853s 5s/step - loss: 1.1066\n",
            "Epoch 13/20\n",
            "172/172 [==============================] - 855s 5s/step - loss: 1.0640\n",
            "Epoch 14/20\n",
            "172/172 [==============================] - 858s 5s/step - loss: 1.0150\n",
            "Epoch 15/20\n",
            "172/172 [==============================] - 863s 5s/step - loss: 0.9619\n",
            "Epoch 16/20\n",
            "172/172 [==============================] - 880s 5s/step - loss: 0.9102\n",
            "Epoch 17/20\n",
            "172/172 [==============================] - 898s 5s/step - loss: 0.8564\n",
            "Epoch 18/20\n",
            "172/172 [==============================] - 905s 5s/step - loss: 0.8042\n",
            "Epoch 19/20\n",
            "172/172 [==============================] - 899s 5s/step - loss: 0.7523\n",
            "Epoch 20/20\n",
            "172/172 [==============================] - 892s 5s/step - loss: 0.7019\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uhRKc12AxT67"
      },
      "source": [
        "#Generate Text\n",
        "\n",
        "class OneStep(tf.keras.Model):\n",
        "  def __init__(self, model, chars_from_ids, ids_from_chars, temperature=1.0):\n",
        "    super().__init__()\n",
        "    self.temperature = temperature\n",
        "    self.model = model\n",
        "    self.chars_from_ids = chars_from_ids\n",
        "    self.ids_from_chars = ids_from_chars\n",
        "\n",
        "    # Create a mask to prevent \"\" or \"[UNK]\" from being generated.\n",
        "    skip_ids = self.ids_from_chars(['', '[UNK]'])[:, None]\n",
        "    sparse_mask = tf.SparseTensor(\n",
        "        # Put a -inf at each bad index.\n",
        "        values=[-float('inf')]*len(skip_ids),\n",
        "        indices=skip_ids,\n",
        "        # Match the shape to the vocabulary\n",
        "        dense_shape=[len(ids_from_chars.get_vocabulary())])\n",
        "    self.prediction_mask = tf.sparse.to_dense(sparse_mask)\n",
        "\n",
        "  @tf.function\n",
        "  def generate_one_step(self, inputs, states=None):\n",
        "    # Convert strings to token IDs.\n",
        "    input_chars = tf.strings.unicode_split(inputs, 'UTF-8')\n",
        "    input_ids = self.ids_from_chars(input_chars).to_tensor()\n",
        "\n",
        "    # Run the model.\n",
        "    # predicted_logits.shape is [batch, char, next_char_logits]\n",
        "    predicted_logits, states = self.model(inputs=input_ids, states=states,\n",
        "                                          return_state=True)\n",
        "    # Only use the last prediction.\n",
        "    predicted_logits = predicted_logits[:, -1, :]\n",
        "    predicted_logits = predicted_logits/self.temperature\n",
        "    # Apply the prediction mask: prevent \"\" or \"[UNK]\" from being generated.\n",
        "    predicted_logits = predicted_logits + self.prediction_mask\n",
        "\n",
        "    # Sample the output logits to generate token IDs.\n",
        "    predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n",
        "    predicted_ids = tf.squeeze(predicted_ids, axis=-1)\n",
        "\n",
        "    # Convert from token ids to characters\n",
        "    predicted_chars = self.chars_from_ids(predicted_ids)\n",
        "\n",
        "    # Return the characters and model state.\n",
        "    return predicted_chars, states"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rpExwNAQxXoP"
      },
      "source": [
        "one_step_model = OneStep(model, chars_from_ids, ids_from_chars)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w4Z0fRzQxeWK",
        "outputId": "6b7ad123-0db5-43ba-956f-a6fa2fe360b6"
      },
      "source": [
        "start = time.time()\n",
        "states = None\n",
        "next_char = tf.constant(['ROMEO:'])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(1000):\n",
        "  next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
        "  result.append(next_char)\n",
        "\n",
        "result = tf.strings.join(result)\n",
        "end = time.time()\n",
        "print(result[0].numpy().decode('utf-8'), '\\n\\n' + '_'*80)\n",
        "print('\\nRun time:', end - start)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ROMEO:\n",
            "'Tis for my liege, what he would fetch her sister\n",
            "save the Four Rosaline, whom then, of wash your body\n",
            "to such as often and the heavy back:\n",
            "'Tis more than you can harment me;\n",
            "And if Letis another day hath power to this?\n",
            "\n",
            "BAPTISTA:\n",
            "Good night, dead call, at least,\n",
            "And the nobicilier of his useal music at behavior:\n",
            "And so come to her recompension on Fortun,\n",
            "Where being patterly as a prisoner,\n",
            "O, but what is not Rutland more fierced and great love?\n",
            "\n",
            "BIANCA:\n",
            "Now justice men, my lord, let me an end;\n",
            "Some say he has in possess.\n",
            "\n",
            "First Conspirator:\n",
            "So, mighty soul.\n",
            "\n",
            "TYRREL:\n",
            "My lord, your souls not build to die.\n",
            "\n",
            "CAMILLO:\n",
            "\n",
            "BIANCA:\n",
            "If we will be, love them most aumed\n",
            "To bloody things were from that have before thee; make up my brother:\n",
            "Nor shrinks it open, let me lay in bribre.\n",
            "Best fortain our mildness in the bind this dreams,\n",
            "That he consents in mine arms. I have been down thee:\n",
            "That's the way into your ablector; selute haste,\n",
            "And sligh their smiles on the earth's company\n",
            "As to your royal gr \n",
            "\n",
            "________________________________________________________________________________\n",
            "\n",
            "Run time: 2.9788239002227783\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jSvQUCgKxmDQ",
        "outputId": "1a4427ff-446f-4261-a1f2-1bf04025f71f"
      },
      "source": [
        "start = time.time()\n",
        "states = None\n",
        "next_char = tf.constant(['ROMEO:', 'ROMEO:', 'ROMEO:', 'ROMEO:', 'ROMEO:'])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(1000):\n",
        "  next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
        "  result.append(next_char)\n",
        "\n",
        "result = tf.strings.join(result)\n",
        "end = time.time()\n",
        "print(result, '\\n\\n' + '_'*80)\n",
        "print('\\nRun time:', end - start)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(\n",
            "[b\"ROMEO:\\nStir, there is such as wheels sit with their frailing bed.\\nHer sux his poiton: thou hast slept: then, I would kiss\\nAt frame you something new.\\nO manage Kents, the temple near 'we?\\nOne world! she was the devil's commandered\\nthat he hath lost either of your souls\\nTo strew him up that hungman moved his head.\\n\\nDUKE OF YORK:\\nIt shall be profess, but in a serpen of all nister gates:\\nNow sways it with me an apprehension.\\nGood nakes, you the devil's death,\\nWhich then comes on my brother is come along.\\n\\nDUKE OF YORK:\\nIt were none.\\n\\nPETRUCHIO:\\nCome but thee; there is my life may nine.\\n\\nPAULINA:\\nEre sent you supory.\\n\\nKING HENRY VI:\\nFor what, lies, well we here: God keep her use we heard\\nMore than a pipour of his love.\\n\\nHASTINGS:\\nI think, I hear, sir, by the regal spirit,\\n'Be seem to bed to murder: mine is a traitor of the world;\\nAnd yet shall sting me that e'er made thee back.\\n\\nPAULINA:\\nSir, here some shall be well.\\n\\nBAPTISTA:\\nAy, good my lord; there was a man.\\n\\nMENENIUS:\\nDone her hence!\\nGod sav\"\n",
            " b\"ROMEO:\\nMore of him that he comes.\\n\\nMENENIUS:\\nBut indeed I'll be the yively love.\\n\\nMERCUTIO:\\nI had a landering slave, there's none of your desires\\nAre therein were heavy in your flatterately,\\nFor in his blood to surpose this in thine oaths.\\nO, the beare love hath made in my mischance,\\nFor then have expose their bosom in particular rid.\\nBut nature she hath presently as big as two\\nLord Clifford and Romeo, will your king,\\nYet smiling in blood's privatels, my understanding is so hurbid up.\\n\\nDUCHESS OF YORK:\\nThus can you so ordend that maids them, would\\nI'ld welcome human and her suit\\nDo come the supposed usurpe: when\\nthey have touch'd that guest amends, and that beyond their\\ntrue duty! she hath kept my tale in words,\\nThen most lost as well as strange.\\nIn his delay, sweet love, for Edward to the time.\\nFare yet me, live some servant unto my understand bed:\\nAnd you shall hear; you know our trial that flight,\\nAs much in leavine shapes, woe not thyself--\\nThe earldom of Hereford on her.\\n\\nPETRUCHIO:\\nCom\"\n",
            " b\"ROMEO:\\nLet's be fetch'd.\\n\\nLEONTES:\\nDo must grant you what.\\n\\nGONZALO:\\nMarch, thou\\nlie, nor attention; here is here, this dear delight'st to arm,\\nAnd for our cousin Warwick, and is no longer.\\n\\nQUEEN:\\nAnd then unwas to dance him, privately\\nstraight, beguile the law, thou shalt be\\nThe usurping proffering sways or walt:\\nGive music, in my old borough to do in word;\\nO, special whiles, the noble of his suit:\\nDo thou but close our nuptial devices?\\n\\nThird Citizen:\\nWe shall be what a bow,\\nI may be young madman, and thou most courteous,\\nThat feel together is my liege,\\nEre to disprague himself will be my way.\\n\\nISABELLA:\\nTo burn with maid the like half of much of more.\\n\\nPOMPEY:\\nKnows most his mother?\\n\\nPERDITA:\\nPardon, come forth;\\nUntil my heart were fortunate hath not.\\n\\nTYON ELIZABETH:\\nWhat say you, her? thy son's another?\\n\\nCAMILLO:\\nHeaven let's away o' the point,\\nI heard you see, 'tis shrew as a coffin'd scorn: poor love, so his blace?\\nThrough at the point of body hath brought\\nHave full use thee.\\n\\nMARCIU\"\n",
            " b\"ROMEO:\\nWhy, here's a very calger-guard, her brothers both:\\nGod saves thy lord and words or come, slander up with cisio.\\nWill he not hear him from brawled.\\nWhate'er thou hast not dance? 'tis that I love him.\\n\\nPROSPERO:\\nHow now you know\\nHis grace to grief, most grace, muster up in regrath,\\nChrist is here landed, point of woold,\\nThink thee in some other parts hath made:\\nNor the elder, sir, whom God he knows,\\nOur subjects but eat who else besolved\\nFalse to Beridly and my legs beyond\\nThe time we splike thee. Hoo!\\n\\nRICHMOND:\\nGreat God of much golden soldier,\\nNor never sound the old proceedings.\\nRene,--favour in my anchor, weeping, with a word again\\nTo ach from France to see thee here; here will have\\nMore mother shall be mild, being there sickness.\\n\\nQUEEN ELIZABETH:\\nThank, let me haz. Ah, my\\nThurse is yours\\nShall make a pipeful breath, yet she kissing.\\nSound true, from the flattering temples of my grief,\\nTouches me deep and daughter: therefore be prosperous:\\ndeach it ere entreaties: let us slain\\nFo\"\n",
            " b\"ROMEO:\\nSome came we a lover.\\n\\nPAULINA:\\nFear none: why call'd fearful?\\nMy love speaks I have waste my remembrance of\\ntrue to the consuls by a pedlar: their admiring,\\nNor yoke where more inevort of my fear?\\nWhere be the seponal stinging Edward?\\n\\nHERMere quitter?\\n\\nELBOW:\\nNay, I would he well perceive\\nUnto a mortal pointed brake of concludings;\\nOr, seeking out decreed, and be freely\\nDuke of all issue.\\n\\nCOMINIUS:\\nThe seasons have marquing issued.\\n\\nMARIANA:\\nI know it;\\nIf any shame but me. A honey mum! I call them forgive;\\nAnd tell what wouldst dare brain and courage,\\nAnd show my master kind in blind into\\nI mean to make her; but this lance's too.\\nNay, madam, I never wast the Bustice;\\nEven of our leisure in a traitor rie on\\nWelt the same horse; where is he. Beard, bear in\\narviless: be Derivime, he that run fairly but a smock\\nOf mack cozplexs, they are past a pipes importune,\\nWhich wash'd with better restulent of your honour!\\n\\nARIEL:\\nO, the wager seems.\\n\\nPROSPERO:\\nSo precupel his house; sir? far is i\"], shape=(5,), dtype=string) \n",
            "\n",
            "________________________________________________________________________________\n",
            "\n",
            "Run time: 5.061970233917236\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e-qGO9UUxqG0",
        "outputId": "efd9aaf9-04a2-4087-b112-1e6c442992ae"
      },
      "source": [
        "tf.saved_model.save(one_step_model, 'one_step')\n",
        "one_step_reloaded = tf.saved_model.load('one_step')#exporting the model\n",
        "\n",
        "states = None\n",
        "next_char = tf.constant(['ROMEO:'])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(100):\n",
        "  next_char, states = one_step_reloaded.generate_one_step(next_char, states=states)\n",
        "  result.append(next_char)\n",
        "\n",
        "print(tf.strings.join(result)[0].numpy().decode(\"utf-8\"))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Skipping full serialization of Keras layer <__main__.OneStep object at 0x7f065c4c4f50>, because it is not built.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as gru_cell_layer_call_and_return_conditional_losses, gru_cell_layer_call_fn, gru_cell_layer_call_fn, gru_cell_layer_call_and_return_conditional_losses, gru_cell_layer_call_and_return_conditional_losses while saving (showing 5 of 5). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as gru_cell_layer_call_and_return_conditional_losses, gru_cell_layer_call_fn, gru_cell_layer_call_fn, gru_cell_layer_call_and_return_conditional_losses, gru_cell_layer_call_and_return_conditional_losses while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: one_step/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: one_step/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:5 out of the last 5 calls to <function recreate_function.<locals>.restored_function_body at 0x7f065a0ad830> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:5 out of the last 5 calls to <function recreate_function.<locals>.restored_function_body at 0x7f065a0ad830> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:6 out of the last 6 calls to <function recreate_function.<locals>.restored_function_body at 0x7f065a0ad830> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:6 out of the last 6 calls to <function recreate_function.<locals>.restored_function_body at 0x7f065a0ad830> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "ROMEO:\n",
            "What fates well,\n",
            "'I was adventure.\n",
            "\n",
            "POMPEY:\n",
            "I son to him, and he shall quit like liberty.\n",
            "\n",
            "GLOUCEST\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ieC2hnn-xxzl"
      },
      "source": [
        "class CustomTraining(MyModel):\n",
        "  @tf.function\n",
        "  def train_step(self, inputs):\n",
        "      inputs, labels = inputs\n",
        "      with tf.GradientTape() as tape:\n",
        "          predictions = self(inputs, training=True)\n",
        "          loss = self.loss(labels, predictions)\n",
        "      grads = tape.gradient(loss, model.trainable_variables)\n",
        "      self.optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "\n",
        "      return {'loss': loss}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AZt8xMLrx94S"
      },
      "source": [
        "model = CustomTraining(\n",
        "    vocab_size=len(ids_from_chars.get_vocabulary()),\n",
        "    embedding_dim=embedding_dim,\n",
        "    rnn_units=rnn_units)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mz4il-4Vx_UW"
      },
      "source": [
        "model.compile(optimizer = tf.keras.optimizers.Adam(),\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3mtnpJjHyA1d",
        "outputId": "a9c8eea3-c875-4f7e-a5bd-55fdbab036b0"
      },
      "source": [
        "model.fit(dataset, epochs=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "172/172 [==============================] - 885s 5s/step - loss: 2.7245\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f065b473450>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yYJPyRe8yCUS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "73d7ccab-1da2-48aa-e061-ba1cf4924629"
      },
      "source": [
        "EPOCHS = 10\n",
        "\n",
        "mean = tf.metrics.Mean()\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    start = time.time()\n",
        "\n",
        "    mean.reset_states()\n",
        "    for (batch_n, (inp, target)) in enumerate(dataset):\n",
        "        logs = model.train_step([inp, target])\n",
        "        mean.update_state(logs['loss'])\n",
        "\n",
        "        if batch_n % 50 == 0:\n",
        "            template = f\"Epoch {epoch+1} Batch {batch_n} Loss {logs['loss']:.4f}\"\n",
        "            print(template)\n",
        "\n",
        "    # saving (checkpoint) the model every 5 epochs\n",
        "    if (epoch + 1) % 5 == 0:\n",
        "        model.save_weights(checkpoint_prefix.format(epoch=epoch))\n",
        "\n",
        "    print()\n",
        "    print(f'Epoch {epoch+1} Loss: {mean.result().numpy():.4f}')\n",
        "    print(f'Time taken for 1 epoch {time.time() - start:.2f} sec')\n",
        "    print(\"_\"*80)\n",
        "\n",
        "model.save_weights(checkpoint_prefix.format(epoch=epoch))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 Batch 0 Loss 2.2131\n",
            "Epoch 1 Batch 50 Loss 2.0703\n",
            "Epoch 1 Batch 100 Loss 1.9890\n",
            "Epoch 1 Batch 150 Loss 1.8595\n",
            "\n",
            "Epoch 1 Loss: 1.9961\n",
            "Time taken for 1 epoch 884.78 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 2 Batch 0 Loss 1.8008\n",
            "Epoch 2 Batch 50 Loss 1.7652\n",
            "Epoch 2 Batch 100 Loss 1.7051\n",
            "Epoch 2 Batch 150 Loss 1.6534\n",
            "\n",
            "Epoch 2 Loss: 1.7176\n",
            "Time taken for 1 epoch 879.39 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 3 Batch 0 Loss 1.6274\n",
            "Epoch 3 Batch 50 Loss 1.5706\n",
            "Epoch 3 Batch 100 Loss 1.5575\n",
            "Epoch 3 Batch 150 Loss 1.5669\n",
            "\n",
            "Epoch 3 Loss: 1.5560\n",
            "Time taken for 1 epoch 875.80 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 4 Batch 0 Loss 1.5081\n",
            "Epoch 4 Batch 50 Loss 1.5033\n",
            "Epoch 4 Batch 100 Loss 1.4758\n",
            "Epoch 4 Batch 150 Loss 1.4960\n",
            "\n",
            "Epoch 4 Loss: 1.4571\n",
            "Time taken for 1 epoch 881.18 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 5 Batch 0 Loss 1.4321\n",
            "Epoch 5 Batch 50 Loss 1.3376\n",
            "Epoch 5 Batch 100 Loss 1.3632\n",
            "Epoch 5 Batch 150 Loss 1.3377\n",
            "\n",
            "Epoch 5 Loss: 1.3880\n",
            "Time taken for 1 epoch 873.58 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 6 Batch 0 Loss 1.3178\n",
            "Epoch 6 Batch 50 Loss 1.3129\n",
            "Epoch 6 Batch 100 Loss 1.3255\n",
            "Epoch 6 Batch 150 Loss 1.3132\n",
            "\n",
            "Epoch 6 Loss: 1.3361\n",
            "Time taken for 1 epoch 870.37 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 7 Batch 0 Loss 1.3203\n",
            "Epoch 7 Batch 50 Loss 1.2800\n",
            "Epoch 7 Batch 100 Loss 1.2959\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NXRP6upVyETb"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}